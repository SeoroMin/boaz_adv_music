{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"text_augmentation.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[],"mount_file_id":"1lexdv8e8zY1QYgzZCdn7sgy0ExD-eh5m","authorship_tag":"ABX9TyMVX/s5CbqomPwit1LtYd0j"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"7jqVOPzBryhP"},"outputs":[],"source":["# 구글드라이브 연동\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# 데이터셋 불러오기\n","import pandas as pd\n","data = pd.read_excel('/content/drive/MyDrive/데캡디/감성대화/Training/train.xlsx')\n","\n","# 해당 column만 추출\n","df = data[ ['감정_대분류', '사람문장1'] ]\n","\n","df.head()"],"metadata":{"id":"LLwQwjzS-4js"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 합치기\n","new_data = df[['사람문장1', '감정_대분류']]\n","new_data = new_data.rename(columns={'사람문장1':'content', '감정_대분류':'label'})\n","new_data.head()"],"metadata":{"id":"S7Uv4GeV_RRm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import random\n","import pickle\n","import re\n","\n","wordnet = {}\n","with open(\"/content/drive/MyDrive/데캡디/wordnet.pickle\", \"rb\") as f:\n","\twordnet = pickle.load(f)\n"," "],"metadata":{"id":"S3a9QS6w_d62"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 한글만 남기고 나머지는 삭제\n","def get_only_hangul(line):\n","\tparseText= re.compile('/ ^[ㄱ-ㅎㅏ-ㅣ가-힣]*$/').sub('',line)\n","\n","\treturn parseText\n","\n","\n","\n","########################################################################\n","# Synonym replacement\n","# Replace n words in the sentence with synonyms from wordnet\n","########################################################################\n","def synonym_replacement(words, n):\n","\tnew_words = words.copy()\n","\trandom_word_list = list(set([word for word in words]))\n","\trandom.shuffle(random_word_list)\n","\tnum_replaced = 0\n","\tfor random_word in random_word_list:\n","\t\tsynonyms = get_synonyms(random_word)\n","\t\tif len(synonyms) >= 1:\n","\t\t\tsynonym = random.choice(list(synonyms))\n","\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n","\t\t\tnum_replaced += 1\n","\t\tif num_replaced >= n:\n","\t\t\tbreak\n","\n","\tif len(new_words) != 0:\n","\t\tsentence = ' '.join(new_words)\n","\t\tnew_words = sentence.split(\" \")\n","\n","\telse:\n","\t\tnew_words = \"\"\n","\n","\treturn new_words\n","\n","\n","def get_synonyms(word):\n","\tsynomyms = []\n","\n","\ttry:\n","\t\tfor syn in wordnet[word]:\n","\t\t\tfor s in syn:\n","\t\t\t\tsynomyms.append(s)\n","\texcept:\n","\t\tpass\n","\n","\treturn synomyms\n","\n","########################################################################\n","# Random deletion\n","# Randomly delete words from the sentence with probability p\n","########################################################################\n","def random_deletion(words, p):\n","\tif len(words) == 1:\n","\t\treturn words\n","\n","\tnew_words = []\n","\tfor word in words:\n","\t\tr = random.uniform(0, 1)\n","\t\tif r > p:\n","\t\t\tnew_words.append(word)\n","\n","\tif len(new_words) == 0:\n","\t\trand_int = random.randint(0, len(words)-1)\n","\t\treturn [words[rand_int]]\n","\n","\treturn new_words\n","\n","########################################################################\n","# Random swap\n","# Randomly swap two words in the sentence n times\n","########################################################################\n","def random_swap(words, n):\n","\tnew_words = words.copy()\n","\tfor _ in range(n):\n","\t\tnew_words = swap_word(new_words)\n","\n","\treturn new_words\n","\n","def swap_word(new_words):\n","\trandom_idx_1 = random.randint(0, len(new_words)-1)\n","\trandom_idx_2 = random_idx_1\n","\tcounter = 0\n","\n","\twhile random_idx_2 == random_idx_1:\n","\t\trandom_idx_2 = random.randint(0, len(new_words)-1)\n","\t\tcounter += 1\n","\t\tif counter > 3:\n","\t\t\treturn new_words\n","\n","\tnew_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]\n","\treturn new_words\n","\n","########################################################################\n","# Random insertion\n","# Randomly insert n words into the sentence\n","########################################################################\n","def random_insertion(words, n):\n","\tnew_words = words.copy()\n","\tfor _ in range(n):\n","\t\tadd_word(new_words)\n","\t\n","\treturn new_words\n","\n","\n","def add_word(new_words):\n","\tsynonyms = []\n","\tcounter = 0\n","\twhile len(synonyms) < 1:\n","\t\tif len(new_words) >= 1:\n","\t\t\trandom_word = new_words[random.randint(0, len(new_words)-1)]\n","\t\t\tsynonyms = get_synonyms(random_word)\n","\t\t\tcounter += 1\n","\t\telse:\n","\t\t\trandom_word = \"\"\n","\n","\t\tif counter >= 10:\n","\t\t\treturn\n","\t\t\n","\trandom_synonym = synonyms[0]\n","\trandom_idx = random.randint(0, len(new_words)-1)\n","\tnew_words.insert(random_idx, random_synonym)"],"metadata":{"id":"kLTL1Sqd_pUv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def EDA(sentence, alpha_sr=0.1, alpha_ri=0.1, alpha_rs=0.1, p_rd=0.1, num_aug=5):\n","\tsentence = get_only_hangul(sentence)\n","\twords = sentence.split(' ')\n","\twords = [word for word in words if word is not \"\"]\n","\tnum_words = len(words)\n","\n","\taugmented_sentences = []\n","\tnum_new_per_technique = int(num_aug/4) + 1\n","\n","\tn_sr = max(1, int(alpha_sr*num_words))\n","\tn_ri = max(1, int(alpha_ri*num_words))\n","\tn_rs = max(1, int(alpha_rs*num_words))\n","\n","\t# rd\n","\tfor _ in range(num_new_per_technique):\n","\t\ta_words = random_deletion(words, p_rd)\n","\t\taugmented_sentences.append(\" \".join(a_words))\n","  \n","\n","    # sr\n","\tfor _ in range(num_new_per_technique):\n","\t\ta_words = synonym_replacement(words, n_sr)\n","\t\taugmented_sentences.append(' '.join(a_words))\n","  \n","    # ri\n","\tfor _ in range(num_new_per_technique):\n","\t\ta_words = random_insertion(words, n_ri)\n","\t\taugmented_sentences.append(' '.join(a_words))\n","  \n","    # rs\n","\tfor _ in range(num_new_per_technique):\n","\t\ta_words = random_swap(words, n_rs)\n","\t\taugmented_sentences.append(\" \".join(a_words))\n","  \t\n","\n","\t\n","\n","\taugmented_sentences = [get_only_hangul(sentence) for sentence in augmented_sentences]\n","\trandom.shuffle(augmented_sentences)\n","\n","\tif num_aug >= 1:\n","\t\taugmented_sentences = augmented_sentences[:num_aug]\n","\telse:\n","\t\tkeep_prob = num_aug / len(augmented_sentences)\n","\t\taugmented_sentences = [s for s in augmented_sentences if random.uniform(0, 1) < keep_prob]\n","\n","\taugmented_sentences.append(sentence)\n","\n","\treturn augmented_sentences"],"metadata":{"id":"zu7KWsc8_sGn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_1 = new_data[new_data['label']=='분노']\n","df_2 = new_data[new_data['label']=='슬픔']\n","df_3 = new_data[new_data['label']=='불안']\n","df_4 = new_data[new_data['label']=='상처']\n","df_5 = new_data[new_data['label']=='당황']\n","df_6 = new_data[new_data['label']=='기쁨']"],"metadata":{"id":"hs_-Eglf_u4r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_1_aug = df_1['content'].apply(EDA)\n","df_2_aug = df_2['content'].apply(EDA)\n","df_3_aug = df_3['content'].apply(EDA)\n","df_4_aug = df_4['content'].apply(EDA)\n","df_5_aug = df_5['content'].apply(EDA)\n","df_6_aug = df_6['content'].apply(EDA)"],"metadata":{"id":"tY3yCRqcAT49"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["aug_1 = []\n","for i in range(len(df_1_aug)):\n","  for j in range(len(df_1_aug.iloc[i])):\n","    aug_1.append(df_1_aug.iloc[i][j])\n","\n","aug_2 = []\n","for i in range(len(df_2_aug)):\n","  for j in range(len(df_2_aug.iloc[i])):\n","    aug_2.append(df_2_aug.iloc[i][j])\n","\n","aug_3 = []\n","for i in range(len(df_3_aug)):\n","  for j in range(len(df_3_aug.iloc[i])):\n","    aug_3.append(df_3_aug.iloc[i][j])\n","\n","aug_4 = []\n","for i in range(len(df_4_aug)):\n","  for j in range(len(df_4_aug.iloc[i])):\n","    aug_4.append(df_4_aug.iloc[i][j])\n","\n","aug_5 = []\n","for i in range(len(df_5_aug)):\n","  for j in range(len(df_5_aug.iloc[i])):\n","    aug_5.append(df_5_aug.iloc[i][j])\n","\n","aug_6 = []\n","for i in range(len(df_6_aug)):\n","  for j in range(len(df_6_aug.iloc[i])):\n","    aug_6.append(df_6_aug.iloc[i][j])"],"metadata":{"id":"ZsZB23GqAept"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_augmentation_1 = pd.DataFrame({'content': aug_1,\n","             'label': '분노'})\n","df_augmentation_2 = pd.DataFrame({'content': aug_2,\n","             'label': '슬픔'})\n","df_augmentation_3 = pd.DataFrame({'content': aug_3,\n","             'label': '불안'})\n","df_augmentation_4 = pd.DataFrame({'content': aug_4,\n","             'label': '상처'})\n","df_augmentation_5 = pd.DataFrame({'content': aug_5,\n","             'label': '당황'})\n","df_augmentation_6 = pd.DataFrame({'content': aug_6,\n","             'label': '기쁨'})\n"],"metadata":{"id":"dC4P139aA-w-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_augmentation_1[:6]"],"metadata":{"id":"bOmi-m9nBRu4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = pd.concat([df_augmentation_1, df_augmentation_2, df_augmentation_3, df_augmentation_4, df_augmentation_5, df_augmentation_6])"],"metadata":{"id":"sjQ07TysBUgj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data"],"metadata":{"id":"qUry9HMZBcDv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = data.sample(frac=1)"],"metadata":{"id":"L2qxS1FcBdrr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['label'].value_counts()"],"metadata":{"id":"iRi2LQsrBlFv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"7vWyVc80BmAB"},"execution_count":null,"outputs":[]}]}